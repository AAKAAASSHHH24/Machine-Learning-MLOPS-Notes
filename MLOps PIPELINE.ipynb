{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2cb56bd",
   "metadata": {},
   "source": [
    "# MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347d0c0",
   "metadata": {},
   "source": [
    "MLOps stands for Machine Learning Operations.\n",
    "\n",
    "MLOps is focused on streamlining the process of deploying machine learning models to production, and then maintaining and monitoring them. \n",
    "MLOps is a collaborative function, often consisting of data scientists, ML engineers, and DevOps engineers. \n",
    "The word MLOps is a compound of two different fields i.e. machine learning and DevOps from software engineering.\n",
    "\n",
    "\n",
    "An optimal MLOps experience is one where Machine Learning assets are treated consistently with all other software assets within a CI/CD environment. Machine Learning models can be deployed alongside the services that wrap them and the services that consume them as part of a unified release process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acf675",
   "metadata": {},
   "source": [
    "While MLOps started as a set of best practices, it is slowly evolving into an independent approach to ML lifecycle management. MLOps applies to the entire lifecycle - from integrating with model generation (software development lifecycle and continuous integration/continuous delivery), orchestration, and deployment, to health, diagnostics, governance, and business metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa3fd7",
   "metadata": {},
   "source": [
    "Why MLOps?\n",
    "There are many goals enterprises want to achieve through MLOps. Some of the common ones are:\n",
    "\n",
    "Automation\n",
    "\n",
    "Scalability\n",
    "\n",
    "Reproducibility\n",
    "\n",
    "Monitoring\n",
    "\n",
    "Governance\n",
    "\n",
    "\n",
    "MLOps vs DevOps\n",
    "\n",
    "DevOps is an iterative approach to shipping software applications into production. MLOps borrows the same principles to take machine learning models to production. Either Devops or MLOps, the eventual objective is higher quality and control of software applications/ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beccd238",
   "metadata": {},
   "source": [
    "## MLOps stages and their Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048d1be",
   "metadata": {},
   "source": [
    "Development & Experimentation (ML algorithms, new ML models) >>>>> Source code for pipelines: Data extraction, validation, preparation, model training, model evaluation, model testing\n",
    "\n",
    "Pipeline Continuous Integration (Build source code and run tests) >>>>> Pipeline components to be deployed: packages and executables.\n",
    "\n",
    "Pipeline Continuous Delivery (Deploy pipelines to the target environment) >>>>> Deployed pipeline with new implementation of the model.\n",
    "\n",
    "Automated Triggering (Pipeline is automatically executed in production. Schedule or trigger are used) >>>>> Trained model that is stored in the model registry.\n",
    "\n",
    "Model Continuous Delivery (Model serving for prediction) >>>>> Deployed model prediction service (e.g. model exposed as REST API)\n",
    "\n",
    "Monitoring (Collecting data about the model performance on live data) >>>>> Trigger to execute the pipeline or to start a new experiment cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe366a",
   "metadata": {},
   "source": [
    "## MLOps setup components\n",
    "\n",
    "Source Control >>>>> Versioning the Code, Data, and ML Model artifacts.\n",
    "\n",
    "Test & Build Services >>>>> Using CI tools for (1) Quality assurance  for all ML artifacts, and (2) Building packages and executables for pipelines.\n",
    "\n",
    "Deployment Services >>>>> Using CD tools for deploying pipelines to the target environment.\n",
    "\n",
    "Model Registry >>>>> A registry for storing already trained ML models.\n",
    "\n",
    "Feature Store >>>>> Preprocessing input data as features to be consumed in the model training pipeline and during the model serving.\n",
    "\n",
    "ML Metadata Store >>>>> Tracking metadata of model training, for example model name, parameters, training data, test data, and metric results.\n",
    "\n",
    "ML Pipeline Orchestrator >>>>> Automating the steps of the ML experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9902066",
   "metadata": {},
   "source": [
    "## Continuous\n",
    "\n",
    "MLOps is an ML engineering culture that includes the following practices:\n",
    "\n",
    "Continuous Integration (CI) extends the testing and validating code and components by adding testing and validating data and models.\n",
    "\n",
    "Continuous Delivery (CD) concerns with delivery of an ML training pipeline that automatically deploys another the ML model prediction service.\n",
    "\n",
    "Continuous Training (CT) is unique to ML systems property, which automatically retrains ML models for re-deployment.\n",
    "\n",
    "Continuous Monitoring (CM) concerns with monitoring production data and model performance metrics, which are bound to business metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440dc2b",
   "metadata": {},
   "source": [
    "## Versioning\n",
    "\n",
    "The goal of the versioning is to treat ML training scripts, ML models and data sets for model training as first-class citizens in DevOps processes by tracking ML models and data sets with version control systems.Every ML model specification should be versioned in a VCS to make the training of ML models auditable and reproducible. \n",
    "\n",
    "## Changes in ML model or data can be because of these:\n",
    "\n",
    "ML models can be retrained based upon new training data.\n",
    "\n",
    "Models may be retrained based upon new training approaches.\n",
    "\n",
    "Models may be self-learning.\n",
    "\n",
    "Models may degrade over time.\n",
    "\n",
    "Models may be deployed in new applications.\n",
    "\n",
    "Models may be subject to attack and require revision.\n",
    "\n",
    "Models can be quickly rolled back to a previous serving version.\n",
    "\n",
    "Corporate or government compliance may require audit or investigation on both ML model or data, hence we need access to all versions of the productionized ML model.\n",
    "\n",
    "Data may reside across multiple systems.\n",
    "\n",
    "Data may only be able to reside in restricted jurisdictions.\n",
    "\n",
    "Data storage may not be immutable.\n",
    "\n",
    "Data ownership may be a factor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8471d7a5",
   "metadata": {},
   "source": [
    "## Experiments Tracking\n",
    "\n",
    "One way to track multiple experiments is to use different (Git-) branches, each dedicated to the separate experiment. The output of each branch is a trained model.\n",
    "\n",
    "Depending on the selected metric, the trained ML models are compared with each other and the appropriate model is selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd29c4",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "The complete development pipeline includes three essential components:\n",
    "\n",
    "Data pipeline,\n",
    "\n",
    "ML model pipeline, and\n",
    "\n",
    "Application pipeline. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In accordance with this separation we distinguish three scopes for testing in ML systems: \n",
    "\n",
    "tests for features and data, \n",
    "\n",
    "tests for model development, and \n",
    "\n",
    "tests for ML infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457759b",
   "metadata": {},
   "source": [
    "## Features and Data Tests\n",
    "\n",
    "## Data validation:\n",
    "Automatic check for data and features schema/domain.\n",
    "\n",
    "Action: In order to build a schema (domain values), calculate statistics from the training data. This schema can be used as expectation definition or semantic role for input data during training and serving stages.\n",
    "\n",
    "## Features importance test to understand whether new features add a predictive power.\n",
    "Action: Compute correlation coefficient on features columns.\n",
    "\n",
    "Action: Train model with one or two features.\n",
    "\n",
    "Action: Use the subset of features “One of k left out and train a set of different models.\n",
    "Measure data dependencies, inference latency, and RAM usage for each new feature. Compare it with the predictive power of the newly added features.\n",
    "\n",
    "Drop out unused/deprecated features from your infrastructure and document it.\n",
    "\n",
    "\n",
    "Features and data pipelines should be policy-compliant (e.g. GDPR). These requirements should be programmatically checked in both development and production environments.\n",
    "\n",
    "Feature creation code should be tested by unit tests (to capture bugs in features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43960a02",
   "metadata": {},
   "source": [
    "## ***Tests for Reliable Model Development***\n",
    "\n",
    "We need to provide specific testing support for detecting ML-specific errors.\n",
    "\n",
    "Testing ML training should include routines, which verify that algorithms make decisions aligned to business objective. This means that ML algorithm loss metrics (MSE, log-loss, etc.) should correlate with business impact metrics (revenue, user engagement, etc.)\n",
    "\n",
    "Action: The loss metrics - impact metrics relationship, can be measured in small scale A/B testing using an intentionally degraded model.\n",
    "Further reading: Selecting the Right Metric for evaluating Machine Learning Models.\n",
    "\n",
    "\n",
    "## Model staleness test. \n",
    "The model is defined as stale if the trained model does not include up-to-date data and/or does not satisfy the business impact requirements. Stale models can affect the quality of prediction in intelligent software.\n",
    "\n",
    "Action: A/B experiment with older models. Including the range of ages to produce an Age vs. Prediction Quality curve to facilitate the understanding of how often the ML model should be trained.\n",
    "\n",
    "## Assessing the cost of more sophisticated ML models.\n",
    "\n",
    "Action: ML model performance should be compared to the simple baseline ML model (e.g. linear model vs neural network).\n",
    "\n",
    "## Validating performance of a model.\n",
    "\n",
    "It is recommended to separate the teams and procedures collecting the training and test data to remove the dependencies and avoid false methodology propagating from the training set to the test set (source).\n",
    "\n",
    "Action: Use an additional test set, which is disjoint from the training and validation sets. Use this test set only for a final evaluation.\n",
    "\n",
    "## Fairness/Bias/Inclusion testing for the ML model performance.\n",
    "\n",
    "Action: Collect more data that includes potentially under-represented categories.\n",
    "\n",
    "Action: Examine input features if they correlate with protected user categories.\n",
    "\n",
    "Further reading: “Tour of Data Sampling Methods for Imbalanced Classification”\n",
    "\n",
    "## Conventional unit testing for any feature creation, ML model specification code (training) and testing.\n",
    "\n",
    "## Model governance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21140e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11988e80",
   "metadata": {},
   "source": [
    "## ML infrastructure test\n",
    "Training the ML models should be reproducible, which means that training the ML model on the same data should produce identical ML models.>>>>>\n",
    "Diff-testing of ML models relies on deterministic training, which is hard to achieve due to non-convexity of the ML algorithms, random seed generation, or distributed ML model training.\n",
    "Action: determine the non-deterministic parts in the model training code base and try to minimize non-determinism.\n",
    "\n",
    "\n",
    "Test ML API usage. Stress testing.>>>>>>\n",
    "Action: Unit tests to randomly generate input data and training the model for a single optimization step (e.g gradient descent).\n",
    "Action: Crash tests for model training. The ML model should restore from a checkpoint after a mid-training crash.\n",
    "\n",
    "Test the algorithmic correctness.>>>>>>\n",
    "Action: Unit test that it is not intended to completing the ML model training but to train for a few iterations and ensure that loss decreases while training.\n",
    "Avoid: Diff-testing with previously build ML models because such tests are hard to maintain.\n",
    "\n",
    "Integration testing: The full ML pipeline should be integration tested.>>>>>>>>>>>\n",
    "Action: Create a fully automated test that regularly triggers the entire ML pipeline. The test should validate that the data and code successfully finish each stage of training and the resulting ML model performs as expected.\n",
    "All integration tests should be run before the ML model reaches the production environment.\n",
    "\n",
    "Validating the ML model before serving it.>>>>>>>>\n",
    "Action: Setting a threshold and testing for slow degradation in model quality over many versions on a validation set.\n",
    "Action: Setting a threshold and testing for sudden performance drops in a new version of the ML model.\n",
    "\n",
    "ML models are canaried before serving.>>>>>>>\n",
    "Action: Testing that an ML model successfully loads into production serving and the prediction on real-life data is generated as expected.\n",
    "\n",
    "Testing that the model in the training environment gives the same score as the model in the serving environment.>>>>>>>>>\n",
    "Action: The difference between the performance on the holdout data and the “next­day” data. Some difference will always exist. Pay attention to large differences in performance between holdout and “next­day” data because it may indicate that some time-sensitive features cause ML model degradation.\n",
    "Action: Avoid result differences between training and serving environments. Applying a model to an example in the training data and the same example at serving should result in the same prediction. A difference here indicates an engineering error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44889d9b",
   "metadata": {},
   "source": [
    "## Monitoring\n",
    "\n",
    "Once the ML model has been deployed, it need to be monitored to assure that the ML model performs as expected. The following check list for model monitoring activities in production is adopted:\n",
    "\n",
    "Monitor dependency changes throughout the complete pipeline result in notification.\n",
    "Data version change.\n",
    "Changes in source system.\n",
    "Dependencies upgrade.\n",
    "\n",
    "Monitor data invariants in training and serving inputs:>>>>>> Alert if data does not match the schema, which has been specified in the training step.\n",
    "Action: tuning of alerting threshold to ensure that alerts remain useful and not misleading.\n",
    "\n",
    "Monitor whether training and serving features compute the same value.>>>>>>>\n",
    "Since the generation of training and serving features might take place on physically separated locations, we must carefully test that these different code paths are logically identical.\n",
    "Action: (1) Log a sample of the serving traffic. \n",
    "(2) Compute distribution statistics (min, max, avg, values, % of missing values, etc.) on the training features and the sampled serving features and ensure that they match.\n",
    "\n",
    "Monitor the numerical stability of the ML model.>>>>>>>\n",
    "Action: trigger alerts for the occurrence of any NaNs or infinities.\n",
    "\n",
    "Monitor computational performance of an ML system. Both dramatic and slow-leak regression in computational performance should be notified.\n",
    "Action: measure the performance of versions and components of code, data, and model by pre-setting the alerting threshold.\n",
    "Action: collect system usage metrics like GPU memory allocation, network traffic, and disk usage. These metrics are useful for cloud costs estimations.\n",
    "\n",
    "Monitor how stale the system in production is.\n",
    "Measure the age of the model. Older ML models tend to decay in performance.\n",
    "Action: Model monitoring is a continuous process, therefore it is important to identify the elements for monitoring and create a strategy for the model monitoring before reaching production.\n",
    "\n",
    "Monitor the processes of feature generation as they have impact on the model.\n",
    "Action: re-run feature generation on a frequent basis.\n",
    "\n",
    "\n",
    "Monitor degradation of the predictive quality of the ML model on served data. Both dramatic and slow-leak regression in prediction quality should be notified.\n",
    "Degradation might happened due to changes in data or differing code paths, etc.\n",
    "Action: Measure statistical bias in predictions (avg in predictions in a slice of data). Models should have nearly zero bias.\n",
    "Action: If a label is available immediately after the prediction is made, we can measure the quality of prediction in real-time and identify problems.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443287da",
   "metadata": {},
   "source": [
    "## “ML Test Score” System\n",
    "\n",
    "The “ML Test Score” measures the overall readiness of the ML system for production. The final ML Test Score is computed as follows:\n",
    "\n",
    "For each test, half a point is awarded for executing the test manually, with the results documented and distributed.\n",
    "A full point is awarded if the there is a system in place to run that test automatically on a repeated basis.\n",
    "Sum the score of each of the four sections individually: Data Tests, Model Tests, ML Infrastructure Tests, and Monitoring.\n",
    "The final ML Test Score is computed by taking the minimum of the scores aggregated for each of the sections: Data Tests, Model Tests, ML Infrastructure Tests, and Monitoring.\n",
    "\n",
    "After computing the ML Test Score, we can reason about the readiness of the ML system for production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab58c0d",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "Reproducibility in a machine learning workflow means that every phase of either data processing, ML model training, and ML model deployment should produce identical results given the same input.\n",
    "\n",
    "Below we see what the different phases are,the challenges in them and how we aim to achieve reproducibility.\n",
    "\n",
    "(A) Phase:Collecting Data\n",
    "\n",
    "Challenges:Generation of the training data can't be reproduced (e.g due to constant database changes or data loading is random)\n",
    "\n",
    "Method to Ensure Reproducibility: \n",
    "1) Always backup your data.\n",
    "2) Saving a snapshot of the data set (e.g. on the cloud storage).\n",
    "3) Data sources should be designed with timestamps so that a view of the data at any point can be retrieved.\n",
    "4) Data versioning.\n",
    "\n",
    "(B) Phase:Feature Engineering\n",
    "\n",
    "Challenges:\n",
    "Scenarios:\n",
    "1) Missing values are imputed with random or mean values.\n",
    "2) Removing labels based on the percentage of observation.\n",
    "3) Non-deterministic feature extraction methods\n",
    "\n",
    "Method to Ensure Reproducibility: \n",
    "1) Feature generation code should be taken under version control.\n",
    "2) Require reproducibility of the previous step \"Collecting Data\"\n",
    "\n",
    "(C) Model Training / Model Build\n",
    "\n",
    "Challenges\n",
    "Non-determinism\n",
    "\n",
    "Method to Ensure Reproducibility: \n",
    "1) Ensure the order of features is always the same.\n",
    "2) Document and automate feature transformation, such as normalization.\n",
    "3) Document and automate hyperparameter selection.\n",
    "4) For ensemble learning: document and automate the combination of ML models.\n",
    "\n",
    "(D) Model Deployment\n",
    "\n",
    "Challenges\n",
    "1) Training the ML model has been performed with a software version that is different to the production environment.\n",
    "2) The input data, which is required by the ML model is missing in the production environment.\n",
    "\n",
    "Method to Ensure Reproducibility: \n",
    "1) Software versions and dependencies should match the production environment.\n",
    "2) Use a container (Docker) and document its specification, such as image version.\n",
    "3) Ideally, the same programming language is used for training and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae12bc1",
   "metadata": {},
   "source": [
    "## Loosely Coupled Architecture (Modularity)\n",
    "This key architectural property enables teams to easily test and deploy individual components or services even as the organization and the number of systems it operates grow—that is, it allows organizations to increase their productivity as they scale.\n",
    "\n",
    "According to Gene Kim et al., in their book “Accelerate”, “high performance [in software delivery] is possible with all kinds of systems, provided that systems—and the teams that build and maintain them — are loosely coupled. ”\n",
    "\n",
    "Using a loosely coupled architecture affects the extent to which a team can test and deploy their applications on demand, without requiring orchestration with other services. \n",
    "\n",
    "Having a loosely coupled architecture allows your teams to work independently, without relying on other teams for support and services, which in turn enables them to work quickly and deliver value to the organization.\n",
    "\n",
    "Regarding ML-based software systems, \n",
    "\n",
    "it can be more difficult to achieve loose coupling between machine learning components than for traditional software components. ML systems have weak component boundaries in several ways. \n",
    "For example, the outputs of ML models can be used as the inputs to another ML model and such interleaved dependencies might affect one another during training and testing.\n",
    "\n",
    "Basic modularity can be achieved by structuring the machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20775a",
   "metadata": {},
   "source": [
    "## ML-based Software Delivery Metrics\n",
    "\n",
    "Four key metrics that capture the effectivenes of the software development and delivery of elite/high performing organisations:\n",
    "\n",
    "(A) Deployment Frequency,\n",
    "\n",
    "ML Model Deployment Frequency depends on\n",
    "1) Model retraining requirements (ranging from less frequent to online training). Two aspects are crucial for model retraining\n",
    "1.1) Model decay metric.\n",
    "1.2) New data availability.\n",
    "2) The level of automation of the deployment process, which might range between *manual deployment* and *fully automated CI/CD pipeline*.\n",
    "\n",
    "(B) Lead Time for Changes,\n",
    "\n",
    "ML Model Lead Time for Changes depends on\n",
    "1) Duration of the explorative phase in Data Science in order to finalize the ML model for deployment/serving.\n",
    "2) Duration of the ML model training.\n",
    "3) The number and duration of manual steps during the deployment process.\n",
    "\n",
    "(C) Mean Time To Restore (MTTR)\n",
    "\n",
    "ML Model MTTR depends on the number and duration of manually performed model debugging, and model deployment steps. In case, when the ML model should be retrained, then MTTR also depends on the duration of the ML model training. Alternatively, MTTR refers to the duration of the rollback of the ML model to the previous version.\n",
    "\n",
    "(D) Change Fail Percentage.\n",
    "\n",
    "ML Model Change Failure Rate can be expressed in the difference of the currently deployed ML model performance metrics to the previous model's metrics, such as Precision, Recall, F-1, accuracy, AUC, ROC, false positives, etc. ML Model Change Failure Rate is also related to A/B testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff11b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
